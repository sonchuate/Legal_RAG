{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10165151,"sourceType":"datasetVersion","datasetId":6277266}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install  --no-index --find-links=/kaggle/input/vllm063/whl4vllm063 torchvision==0.20.1+cu121\n!pip install  --no-index --find-links=/kaggle/input/vllm063/torch24cu12 torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0\n!pip install  --no-index --find-links=/kaggle/input/vllm063/whl4vllm063 vllm==0.6.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:30:52.794749Z","iopub.execute_input":"2025-03-09T13:30:52.794953Z","iopub.status.idle":"2025-03-09T13:33:47.830641Z","shell.execute_reply.started":"2025-03-09T13:30:52.794932Z","shell.execute_reply":"2025-03-09T13:33:47.829507Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu121\nUninstalling torch-2.5.1+cu121:\n  Successfully uninstalled torch-2.5.1+cu121\nLooking in links: /kaggle/input/vllm063/whl4vllm063\nRequirement already satisfied: torchvision==0.20.1+cu121 in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.20.1+cu121) (1.26.4)\nProcessing /kaggle/input/vllm063/whl4vllm063/torch-2.5.1+cu121-cp310-cp310-linux_x86_64.whl (from torchvision==0.20.1+cu121)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.20.1+cu121) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (2024.12.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from torch==2.5.1->torchvision==0.20.1+cu121)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision==0.20.1+cu121) (1.13.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1->torchvision==0.20.1+cu121) (12.6.85)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision==0.20.1+cu121) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.20.1+cu121) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision==0.20.1+cu121) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.20.1+cu121) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.20.1+cu121) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.20.1+cu121) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.20.1+cu121) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.20.1+cu121) (2024.2.0)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 torch-2.5.1+cu121 triton-3.1.0\nLooking in links: /kaggle/input/vllm063/torch24cu12\nProcessing /kaggle/input/vllm063/torch24cu12/torch-2.4.0+cu121-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/vllm063/torch24cu12/torchvision-0.19.0+cu121-cp310-cp310-linux_x86_64.whl\nProcessing /kaggle/input/vllm063/torch24cu12/torchaudio-2.4.0+cu121-cp310-cp310-linux_x86_64.whl\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\nProcessing /kaggle/input/vllm063/torch24cu12/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (from torch==2.4.0)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\nProcessing /kaggle/input/vllm063/torch24cu12/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch==2.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (11.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.0) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.19.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.19.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.19.0) (2024.2.0)\nInstalling collected packages: triton, nvidia-nccl-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu121\n    Uninstalling torchaudio-2.5.1+cu121:\n      Successfully uninstalled torchaudio-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\nSuccessfully installed nvidia-nccl-cu12-2.20.5 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\nLooking in links: /kaggle/input/vllm063/whl4vllm063\nProcessing /kaggle/input/vllm063/whl4vllm063/vllm-0.6.3-cp38-abi3-manylinux1_x86_64.whl\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (5.9.5)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.2.0)\nRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (1.26.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (4.67.1)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (9.0.0)\nRequirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (4.47.0)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.21.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (3.20.3)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (3.11.12)\nRequirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (1.57.4)\nProcessing /kaggle/input/vllm063/whl4vllm063/uvicorn-0.32.1-py3-none-any.whl (from vllm==0.6.3)\nRequirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (2.11.0a2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (11.0.0)\nRequirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.21.1)\nProcessing /kaggle/input/vllm063/whl4vllm063/prometheus_fastapi_instrumentator-7.0.0-py3-none-any.whl (from vllm==0.6.3)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.9.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/lm_format_enforcer-0.10.6-py3-none-any.whl (from vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/outlines-0.0.46-py3-none-any.whl (from vllm==0.6.3)\nRequirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (4.12.2)\nRequirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (3.17.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/partial_json_parser-0.2.1.1.post4-py3-none-any.whl (from vllm==0.6.3)\nRequirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (24.0.1)\nProcessing /kaggle/input/vllm063/whl4vllm063/msgspec-0.18.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/gguf-0.10.0-py3-none-any.whl (from vllm==0.6.3)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (8.5.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/mistral_common-1.5.1-py3-none-any.whl (from mistral-common[opencv]>=1.4.4->vllm==0.6.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (6.0.2)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.8.0)\nRequirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (2.42.1)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (12.570.86)\nRequirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (2.4.0+cu121)\nRequirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3) (0.19.0+cu121)\nProcessing /kaggle/input/vllm063/whl4vllm063/xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (from vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/fastapi-0.115.6-py3-none-any.whl (from vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/interegular-0.3.3-py37-none-any.whl (from lm-format-enforcer==0.10.6->vllm==0.6.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.3) (24.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.3) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.3) (12.6.85)\nProcessing /kaggle/input/vllm063/whl4vllm063/starlette-0.41.3-py3-none-any.whl (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm==0.6.3)\nRequirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (4.23.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (from vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm==0.6.3)\nRequirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm==0.6.3) (4.10.0.84)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->vllm==0.6.3) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm==0.6.3) (1.3.1)\nProcessing /kaggle/input/vllm063/whl4vllm063/lark-1.2.2-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm==0.6.3)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (1.6.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (3.1.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/diskcache-5.6.3-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm==0.6.3)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (0.60.0)\nRequirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (0.35.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.3) (3.3.1)\nProcessing /kaggle/input/vllm063/whl4vllm063/pycountry-24.6.1-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/pyairports-2.1.1-py3-none-any.whl (from outlines<0.1,>=0.0.43->vllm==0.6.3)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.6.3) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm==0.6.3) (2.29.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3) (8.1.7)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3) (1.1.0)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.3) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3) (2025.1.31)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm==0.6.3) (2024.11.6)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm==0.6.3) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.0->vllm==0.6.3) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (2.4.6)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (25.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3) (1.18.3)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm==0.6.3) (3.21.0)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3) (0.14.0)\nProcessing /kaggle/input/vllm063/whl4vllm063/httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from uvicorn[standard]->vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/python_dotenv-1.0.1-py3-none-any.whl (from uvicorn[standard]->vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from uvicorn[standard]->vllm==0.6.3)\nProcessing /kaggle/input/vllm063/whl4vllm063/watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from uvicorn[standard]->vllm==0.6.3)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.3) (14.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->vllm==0.6.3) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.3) (1.0.7)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm==0.6.3) (0.22.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.70.16)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.3) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->vllm==0.6.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->vllm==0.6.3) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->vllm==0.6.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->vllm==0.6.3) (2024.2.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.3) (0.43.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm==0.6.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->vllm==0.6.3) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.3) (1.17.0)\nInstalling collected packages: pyairports, uvloop, uvicorn, python-dotenv, pycountry, pillow, partial-json-parser, msgspec, lark, interegular, httptools, diskcache, watchfiles, tiktoken, starlette, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, mistral-common, xformers, outlines, gguf, vllm\n  Attempting uninstall: pillow\n    Found existing installation: pillow 11.0.0\n    Uninstalling pillow-11.0.0:\n      Successfully uninstalled pillow-11.0.0\n  Attempting uninstall: tiktoken\n    Found existing installation: tiktoken 0.9.0\n    Uninstalling tiktoken-0.9.0:\n      Successfully uninstalled tiktoken-0.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed diskcache-5.6.3 fastapi-0.115.6 gguf-0.10.0 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 lm-format-enforcer-0.10.6 mistral-common-1.5.1 msgspec-0.18.6 outlines-0.0.46 partial-json-parser-0.2.1.1.post4 pillow-10.4.0 prometheus-fastapi-instrumentator-7.0.0 pyairports-2.1.1 pycountry-24.6.1 python-dotenv-1.0.1 starlette-0.41.3 tiktoken-0.7.0 uvicorn-0.32.1 uvloop-0.21.0 vllm-0.6.3 watchfiles-1.0.3 xformers-0.0.27.post2\nCPU times: user 2.49 s, sys: 582 ms, total: 3.07 s\nWall time: 2min 55s\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport torch\n\nMODELS = [\"Qwen/Qwen2.5-7B-Instruct\",\n         \"Qwen/Qwen2.5-Math-7B-Instruct\"]\nmodel_name = MODELS[1]\nprint('model_name', model_name)\n# Create an LLM.\nllm = LLM(model=model_name,dtype=torch.float16,max_model_len=4000,tensor_parallel_size=2,gpu_memory_utilization= 0.96)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:33:47.831979Z","iopub.execute_input":"2025-03-09T13:33:47.832341Z","iopub.status.idle":"2025-03-09T13:36:43.636777Z","shell.execute_reply.started":"2025-03-09T13:33:47.832306Z","shell.execute_reply":"2025-03-09T13:36:43.635776Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\n","output_type":"stream"},{"name":"stdout","text":"model_name Qwen/Qwen2.5-Math-7B-Instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67469ff399248a388e0da878150ee99"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-09 13:34:09 config.py:1674] Casting torch.bfloat16 to torch.float16.\nINFO 03-09 13:34:18 config.py:887] Defaulting to use mp for distributed inference\nINFO 03-09 13:34:18 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-Math-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b36583d2144281a1c90bbe1a6ba1b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f77be96b0b42768f042f2ec914d6c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f4f19b2879b4610ba69672f957b2572"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e24fea09339344a9a18162cea6915b16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/161 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c30b8be682964167974391612109b05d"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-09 13:34:21 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-09 13:34:21 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 03-09 13:34:22 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-09 13:34:22 selector.py:115] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:22 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:22 selector.py:115] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n","output_type":"stream"},{"name":"stdout","text":"INFO 03-09 13:34:22 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\nINFO 03-09 13:34:23 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 03-09 13:34:23 pynccl.py:63] vLLM is using nccl==2.20.5\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:23 utils.py:1008] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:23 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-09 13:34:24 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-09 13:34:43 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:43 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-09 13:34:43 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7973ed329000>, local_subscribe_port=53213, remote_subscribe_port=None)\nINFO 03-09 13:34:43 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:43 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-Math-7B-Instruct...\nINFO 03-09 13:34:43 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-09 13:34:43 selector.py:115] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:43 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:43 selector.py:115] Using XFormers backend.\nINFO 03-09 13:34:43 weight_utils.py:243] Using model weights format ['*.safetensors']\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:34:43 weight_utils.py:243] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af4a823f38394563ae6ab7b940bfead0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd2b184bd23b425bac3f133518b94167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dddb698a1b54544823d8f481f8e2fa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbee27e58464ed08f57e6acf1410c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d00bf755b1974240a9df591df8f79203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a3ecd5e79f34f38b8bd8c2e34d0d2ae"}},"metadata":{}},{"name":"stdout","text":"INFO 03-09 13:36:03 model_runner.py:1071] Loading model weights took 7.1148 GB\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:36:03 model_runner.py:1071] Loading model weights took 7.1148 GB\nINFO 03-09 13:36:06 distributed_gpu_executor.py:57] # GPU blocks: 10512, # CPU blocks: 9362\nINFO 03-09 13:36:06 distributed_gpu_executor.py:61] Maximum concurrency for 4000 tokens per request: 42.05x\nINFO 03-09 13:36:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 03-09 13:36:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:36:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:36:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 03-09 13:36:43 custom_all_reduce.py:233] Registering 1995 cuda graph addresses\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:36:43 custom_all_reduce.py:233] Registering 1995 cuda graph addresses\nINFO 03-09 13:36:43 model_runner.py:1530] Graph capturing finished in 30 secs.\n\u001b[1;36m(VllmWorkerProcess pid=117)\u001b[0;0m INFO 03-09 13:36:43 model_runner.py:1530] Graph capturing finished in 30 secs.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"prompt = [\n{\n    \"role\": \"system\",\n    \"content\": \"2 baskets of apples have 30 apples. 2 times the number of apples in basket A equals 3 times the number of apples in basket B\"\n},\n{\n    \"role\": \"user\",\n    \"content\": \"1 + 1 = \"\n}\n]\n\nsampling_params = SamplingParams(temperature=0.3,top_p=1,n=1, max_tokens=1028,\n            top_k=1,  \n            seed=777, \n            skip_special_tokens=False)\noutputs = llm.chat(prompt,\n               sampling_params=sampling_params,\n               use_tqdm=False)\n# Print the outputs.\nfor i,output in enumerate(outputs):\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T14:01:16.612449Z","iopub.execute_input":"2025-03-09T14:01:16.612774Z","iopub.status.idle":"2025-03-09T14:01:35.451517Z","shell.execute_reply.started":"2025-03-09T14:01:16.612751Z","shell.execute_reply":"2025-03-09T14:01:35.450737Z"}},"outputs":[{"name":"stdout","text":"To solve the problem, we need to determine the number of apples in each basket. Let's denote the number of apples in basket A as \\( A \\) and the number of apples in basket B as \\( B \\).\n\nWe are given two pieces of information:\n1. The total number of apples in both baskets is 30.\n2. Two times the number of apples in basket A equals three times the number of apples in basket B.\n\nWe can translate these pieces of information into the following equations:\n1. \\( A + B = 30 \\)\n2. \\( 2A = 3B \\)\n\nFirst, we solve the second equation for \\( A \\):\n\\[ 2A = 3B \\]\n\\[ A = \\frac{3B}{2} \\]\n\nNext, we substitute \\( A = \\frac{3B}{2} \\) into the first equation:\n\\[ \\frac{3B}{2} + B = 30 \\]\n\nTo eliminate the fraction, we multiply every term by 2:\n\\[ 2 \\cdot \\frac{3B}{2} + 2 \\cdot B = 2 \\cdot 30 \\]\n\\[ 3B + 2B = 60 \\]\n\\[ 5B = 60 \\]\n\nNow, we solve for \\( B \\):\n\\[ B = \\frac{60}{5} \\]\n\\[ B = 12 \\]\n\nSo, basket B has 12 apples. Next, we substitute \\( B = 12 \\) back into the equation \\( A = \\frac{3B}{2} \\) to find \\( A \\):\n\\[ A = \\frac{3 \\cdot 12}{2} \\]\n\\[ A = \\frac{36}{2} \\]\n\\[ A = 18 \\]\n\nSo, basket A has 18 apples. The problem asks for the value of \\( 1 + 1 \\), which is simply:\n\\[ 1 + 1 = 2 \\]\n\nTherefore, the answer is:\n\\[ \\boxed{2} \\]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -q fastapi nest-asyncio pyngrok uvicorn\n!ngrok config add-authtoken 2os3JxffJZfohsU5s81h8F1mo3u_73pvCdvPCebbB87myHSM8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:19:31.620426Z","iopub.execute_input":"2025-03-09T11:19:31.620669Z","iopub.status.idle":"2025-03-09T11:19:38.079940Z","shell.execute_reply.started":"2025-03-09T11:19:31.620645Z","shell.execute_reply":"2025-03-09T11:19:38.078942Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyngrok import ngrok\n# mở port 8000\nport = 8000\nngrok_tunnel = ngrok.connect(port)\n\n# where we can visit our fastAPI app\nprint('Public URL:', ngrok_tunnel.public_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:19:38.081165Z","iopub.execute_input":"2025-03-09T11:19:38.081531Z","iopub.status.idle":"2025-03-09T11:19:38.611086Z","shell.execute_reply.started":"2025-03-09T11:19:38.081493Z","shell.execute_reply":"2025-03-09T11:19:38.610262Z"}},"outputs":[{"name":"stderr","text":"Exception in thread Thread-8 (_monitor_process):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\", line 139, in _monitor_process\n    self._log_line(self.proc.stdout.readline())\n  File \"/usr/lib/python3.10/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 184: ordinal not in range(128)\n","output_type":"stream"},{"name":"stdout","text":"Public URL: https://1103-34-168-59-30.ngrok-free.app\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# serve trên port 8000\n\n!vllm serve Qwen/Qwen2.5-7B-Instruct --dtype float16 --tensor-parallel-size 2 --gpu-memory-utilization 0.96 --max-model-len 2048","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T11:19:38.612045Z","iopub.execute_input":"2025-03-09T11:19:38.612310Z","iopub.status.idle":"2025-03-09T11:20:21.375629Z","shell.execute_reply.started":"2025-03-09T11:19:38.612287Z","shell.execute_reply":"2025-03-09T11:20:21.374552Z"}},"outputs":[{"name":"stdout","text":"2025-03-09 11:19:42.762836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-09 11:19:42.783824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-09 11:19:42.790114: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\nINFO 03-09 11:19:47 api_server.py:528] vLLM API server version dev\nINFO 03-09 11:19:47 api_server.py:529] args: Namespace(subparser='serve', model_tag='Qwen/Qwen2.5-7B-Instruct', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='Qwen/Qwen2.5-7B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', config_format='auto', dtype='float16', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.96, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, override_neuron_config=None, scheduling_policy='fcfs', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, dispatch_function=<function serve at 0x7ae8b2aeb250>)\nINFO 03-09 11:19:47 api_server.py:166] Multiprocessing frontend to use ipc:///tmp/eac7ac18-2293-42e6-9382-1bf65c10ba17 for IPC Path.\nINFO 03-09 11:19:47 api_server.py:179] Started engine process with PID 1398\nconfig.json: 100%|█████████████████████████████| 663/663 [00:00<00:00, 3.83MB/s]\nWARNING 03-09 11:19:47 config.py:1674] Casting torch.bfloat16 to torch.float16.\n2025-03-09 11:19:51.175799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-09 11:19:51.197001: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-09 11:19:51.203331: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\nWARNING 03-09 11:19:55 config.py:1674] Casting torch.bfloat16 to torch.float16.\nINFO 03-09 11:19:57 config.py:887] Defaulting to use mp for distributed inference\ntokenizer_config.json: 100%|███████████████| 7.30k/7.30k [00:00<00:00, 40.1MB/s]\nvocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 4.53MB/s]\nmerges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 8.39MB/s]\ntokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 27.8MB/s]\nINFO 03-09 11:20:05 config.py:887] Defaulting to use mp for distributed inference\nINFO 03-09 11:20:05 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, mm_processor_kwargs=None)\ngeneration_config.json: 100%|██████████████████| 243/243 [00:00<00:00, 1.73MB/s]\nINFO 03-09 11:20:06 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-09 11:20:06 selector.py:115] Using XFormers backend.\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n2025-03-09 11:20:09.868835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-09 11:20:09.888920: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-09 11:20:09.894993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from vllm.version import __version__ as VLLM_VERSION\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:14 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:14 selector.py:115] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:14 multiproc_worker_utils.py:216] Worker ready; awaiting tasks\nINFO 03-09 11:20:15 utils.py:1008] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 utils.py:1008] Found nccl from library libnccl.so.2\nINFO 03-09 11:20:15 pynccl.py:63] vLLM is using nccl==2.20.5\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-09 11:20:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-09 11:20:15 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c471c23c1c0>, local_subscribe_port=42669, remote_subscribe_port=None)\nINFO 03-09 11:20:15 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 model_runner.py:1060] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=1426)\u001b[0;0m INFO 03-09 11:20:15 selector.py:115] Using XFormers backend.\nINFO 03-09 11:20:15 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-09 11:20:15 selector.py:115] Using XFormers backend.\nProcess SpawnProcess-1:\nERROR 03-09 11:20:16 multiproc_worker_utils.py:117] Worker VllmWorkerProcess pid 1426 died, exit code: -15\nINFO 03-09 11:20:16 multiproc_worker_utils.py:121] Killing local vLLM worker processes\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 392, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 141, in from_engine_args\n    return cls(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__\n    self.engine = LLMEngine(*args,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 335, in __init__\n    self.model_executor = executor_class(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\n    self._init_executor()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 111, in _init_executor\n    self._run_workers(\"load_model\",\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\n    driver_worker_output = driver_worker_method(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 183, in load_model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1062, in load_model\n    self.model = get_model(model_config=self.model_config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n    return loader.load_model(model_config=model_config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 398, in load_model\n    model = _initialize_model(model_config, self.load_config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 175, in _initialize_model\n    return build_model(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 160, in build_model\n    return model_class(config=hf_config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 393, in __init__\n    self.model = Qwen2Model(config, cache_config, quant_config)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 248, in __init__\n    self.start_layer, self.end_layer, self.layers = make_layers(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 407, in make_layers\n    [PPMissingLayer() for _ in range(start_layer)] + [\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 408, in <listcomp>\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 250, in <lambda>\n    lambda prefix: Qwen2DecoderLayer(config=config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 184, in __init__\n    self.mlp = Qwen2MLP(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 69, in __init__\n    self.down_proj = RowParallelLinear(intermediate_size,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 995, in __init__\n    self.quant_method.create_weights(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\", line 122, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__\n    return func(*args, **kwargs)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 66.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 2679 has 12.27 GiB memory in use. Process 7180 has 2.44 GiB memory in use. Of the allocated memory 2.22 GiB is allocated by PyTorch, and 38.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W309 11:20:17.306776158 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\nTraceback (most recent call last):\n  File \"/usr/local/bin/vllm\", line 8, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/scripts.py\", line 195, in main\n    args.dispatch_function(args)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/scripts.py\", line 41, in serve\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 82, in run\n    return loop.run_until_complete(wrapper())\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.10/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 552, in run_server\n    async with build_async_engine_client(args) as engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}